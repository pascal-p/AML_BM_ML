{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jlo4V6MRBEDQ"
   },
   "source": [
    "# First things first\n",
    "Click **File -> Save a copy in Drive** and click **Open in new tab** in the pop-up window to save your progress in Google Drive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WMlNHfVxBEDT"
   },
   "source": [
    "# Expectation-maximization algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "icS4MsxIBEDU"
   },
   "source": [
    "In this assignment, we will derive and implement formulas for Gaussian Mixture Model — one of the most commonly used methods for performing soft clustering of the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jequoJfSBEDV"
   },
   "source": [
    "### Setup\n",
    "Loading auxiliary files and importing the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 256
    },
    "colab_type": "code",
    "id": "ZqZo-y9UBEDX",
    "outputId": "6e4574a1-6d33-4038-c56c-ce649b89b8e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Colab files\n",
      "shred: setup_google_colab.py: failed to open for writing: No such file or directory\n",
      "--2019-03-16 04:16:56--  https://raw.githubusercontent.com/hse-aml/bayesian-methods-for-ml/master/setup_google_colab.py\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1308 (1.3K) [text/plain]\n",
      "Saving to: ‘setup_google_colab.py’\n",
      "\n",
      "setup_google_colab. 100%[===================>]   1.28K  --.-KB/s    in 0s      \n",
      "\n",
      "2019-03-16 04:16:57 (174 MB/s) - ‘setup_google_colab.py’ saved [1308/1308]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "if IN_COLAB:\n",
    "    print(\"Downloading Colab files\")\n",
    "    ! shred -u setup_google_colab.py\n",
    "    ! wget https://raw.githubusercontent.com/hse-aml/bayesian-methods-for-ml/master/setup_google_colab.py -O setup_google_colab.py\n",
    "    import setup_google_colab\n",
    "    setup_google_colab.load_data_week2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "urylZcbeBEDc"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import slogdet, det, solve\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from sklearn.datasets import load_digits\n",
    "from w2_grader import EMGrader\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tP8l9frZBEDf"
   },
   "source": [
    "### Grading\n",
    "We will create a grader instance below and use it to collect your answers. Note that these outputs will be stored locally inside grader and will be uploaded to the platform only after running submitting function in the last part of this assignment. If you want to make a partial submission, you can run that cell anytime you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7Gvy3EOvBEDg"
   },
   "outputs": [],
   "source": [
    "grader = EMGrader()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dL3A2sntBEDj"
   },
   "source": [
    "## Implementing EM for GMM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0xPS_VdpBEDk"
   },
   "source": [
    "For debugging, we will use samples from a Gaussian mixture model with unknown mean, variance, and priors. We also added initial values of parameters for grading purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "colab_type": "code",
    "id": "g9_aOn94BEDl",
    "outputId": "b6de0f23-e759-4e84-b8e2-5b75db35113c"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW8AAAD4CAYAAAAjKGdbAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "samples = np.load('samples.npz')\n",
    "X = samples['data']\n",
    "pi0 = samples['pi0']\n",
    "mu0 = samples['mu0']\n",
    "sigma0 = samples['sigma0']\n",
    "plt.scatter(X[:, 0], X[:, 1], c='grey', s=30)\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AmgeHTCfBEDp"
   },
   "source": [
    "### Reminder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RgAwWi1nBEDq"
   },
   "source": [
    "Remember, that EM algorithm is a coordinate descent optimization of variational lower bound $\\mathcal{L}(\\theta, q) = \\int q(T) \\log\\frac{p(X, T|\\theta)}{q(T)}dT\\to \\max$.\n",
    "\n",
    "<b>E-step</b>:<br>\n",
    "$\\mathcal{L}(\\theta, q) \\to \\max\\limits_{q} \\Leftrightarrow \\mathcal{KL} [q(T) \\,\\|\\, p(T|X, \\theta)] \\to \\min \\limits_{q\\in Q} \\Rightarrow q(T) = p(T|X, \\theta)$<br>\n",
    "<b>M-step</b>:<br> \n",
    "$\\mathcal{L}(\\theta, q) \\to \\max\\limits_{\\theta} \\Leftrightarrow \\mathbb{E}_{q(T)}\\log p(X,T | \\theta) \\to \\max\\limits_{\\theta}$\n",
    "\n",
    "For GMM, $\\theta$ is a set of parameters that consists of mean vectors $\\mu_c$, covariance matrices $\\Sigma_c$ and priors $\\pi_c$ for each component.\n",
    "\n",
    "Latent variables $T$ are indices of components to which each data point is assigned, i.e. $t_i$  is the cluster index for object $x_i$.\n",
    "\n",
    "The joint distribution can be written as follows: $\\log p(T, X \\mid \\theta) =  \\sum\\limits_{i=1}^N \\log p(t_i, x_i \\mid \\theta) = \\sum\\limits_{i=1}^N \\sum\\limits_{c=1}^C q(t_i = c) \\log \\left (\\pi_c \\, f_{\\!\\mathcal{N}}(x_i \\mid \\mu_c, \\Sigma_c)\\right)$,\n",
    "where $f_{\\!\\mathcal{N}}(x \\mid \\mu_c, \\Sigma_c) = \\frac{1}{\\sqrt{(2\\pi)^n|\\boldsymbol\\Sigma_c|}}\n",
    "\\exp\\left(-\\frac{1}{2}({x}-{\\mu_c})^T{\\boldsymbol\\Sigma_c}^{-1}({x}-{\\mu_c})\n",
    "\\right)$ is the probability density function (pdf) of the normal distribution $\\mathcal{N}(x_i \\mid \\mu_c, \\Sigma_c)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ELmi1nAtBEDr"
   },
   "source": [
    "### E-step\n",
    "In this step we need to estimate the posterior distribution over the latent variables with fixed values of parameters: $q_i(t_i) = p(t_i \\mid x_i, \\theta)$. We assume that $t_i$ equals to the cluster index of the true component of the $x_i$ object. To do so we need to compute $\\gamma_{ic} = p(t_i = c \\mid x_i, \\theta)$. Note that $\\sum\\limits_{c=1}^C\\gamma_{ic}=1$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "53FR1RJ9BEDs"
   },
   "source": [
    "<b>Important trick 1:</b> It is important to avoid numerical errors. At some point you will have to compute the formula of the following form: $\\frac{e^{y_i}}{\\sum_j e^{y_j}}$, which is called _softmax_. When you compute exponents of large numbers, some numbers may become infinity. You can avoid this by dividing numerator and denominator by $e^{\\max(y)}$: $\\frac{e^{y_i-\\max(y)}}{\\sum_j e^{y_j - \\max(y)}}$. After this transformation maximum value in the denominator will be equal to one. All other terms will contribute smaller values. So, to compute desired formula you first subtract maximum value from each component in vector $\\mathbf{y}$ and then compute everything else as before.\n",
    "\n",
    "<b>Important trick 2:</b> You will probably need to compute formula of the form $A^{-1}x$ at some point. You would normally inverse $A$ and then multiply it by $x$. A bit faster and more numerically accurate way to do this is to directly solve equation $Ay = x$ by using a special function. Its solution is $y=A^{-1}x$, but the equation $Ay = x$ can be solved by methods which do not explicitely invert the matrix. You can use ```np.linalg.solve``` for this.\n",
    "\n",
    "<b>Other usefull functions: </b> <a href=\"https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.slogdet.html\">```slogdet```</a> and <a href=\"https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.det.html#numpy.linalg.det\">```det```</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KL87Q8-TBEDu"
   },
   "source": [
    "<b>Task 1:</b> Implement E-step for GMM using template below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wCUCaD28BEDw"
   },
   "outputs": [],
   "source": [
    "from scipy.stats import multivariate_normal as mv_norm\n",
    "\n",
    "ENABLED = False\n",
    "\n",
    "def print_shape_of(obj, lbl):\n",
    "  if ENABLED:\n",
    "    print(\" ==> shape of (\" + str(lbl) + \") = \" + str(obj.shape))\n",
    "\n",
    "def E_step(X, pi, mu, sigma):\n",
    "    \"\"\"\n",
    "    Performs E-step on GMM model\n",
    "    Each input is numpy array:\n",
    "    X: (N x d), data points\n",
    "    pi: (C), mixture component weights \n",
    "    mu: (C x d), mixture component means\n",
    "    sigma: (C x d x d), mixture component covariance matrices\n",
    "    \n",
    "    Returns:\n",
    "    gamma: (N x C), probabilities of clusters for objects\n",
    "    \"\"\"\n",
    "    N = X.shape[0] # number of objects\n",
    "    C = pi.shape[0] # number of clusters\n",
    "    d = mu.shape[1] # dimension of each object\n",
    "    gamma = np.zeros((N, C)) # distribution q(T) shape [N=280, C=3]\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "    for ix in range(C):\n",
    "      gamma[:, ix] = pi[ix] * mv_norm.pdf(X, mu[ix, :], sigma[ix])\n",
    "        \n",
    "    # normalize\n",
    "    gamma_norm = np.sum(gamma, axis=1)[:, np.newaxis] # shape [280, 1] instead of just [280, ]\n",
    "    gamma /= gamma_norm\n",
    "    \n",
    "    # assert np.all(np.abs(np.sum(gamma, axis=1) - 1.0) <= 0.000001), \"any line of gamma matrix should sum to 1.0\"\n",
    "    \n",
    "    return gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "xlhktOlMBED1",
    "outputId": "7e9a1af9-621b-4df3-f4ee-ab5026e29d1e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current answer for task Task 1 (E-step) is: 0.5337178741081263\n"
     ]
    }
   ],
   "source": [
    "gamma = E_step(X, pi0, mu0, sigma0)\n",
    "grader.submit_e_step(gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fERrQWSCBED5"
   },
   "source": [
    "### M-step\n",
    "\n",
    "In M-step we need to maximize $\\mathbb{E}_{q(T)}\\log p(X,T | \\theta)$ with respect to $\\theta$. In our model this means that we need to find optimal values of $\\pi$, $\\mu$, $\\Sigma$. To do so, you need to compute the derivatives and \n",
    "set them to zero. You should start by deriving formulas for $\\mu$ as it is the easiest part. Then move on to $\\Sigma$. Here it is crucial to optimize function w.r.t. to $\\Lambda = \\Sigma^{-1}$ and then inverse obtained result. Finaly, to compute $\\pi$, you will need <a href=\"https://www3.nd.edu/~jstiver/FIN360/Constrained%20Optimization.pdf\">Lagrange Multipliers technique</a> to satisfy constraint $\\sum\\limits_{i=1}^{n}\\pi_i = 1$.\n",
    "\n",
    "<br>\n",
    "<b>Important note:</b> You will need to compute derivatives of scalars with respect to matrices. To refresh this technique from previous courses, see <a href=\"https://en.wikipedia.org/wiki/Matrix_calculus\"> wiki article</a> about it . Main formulas of matrix derivatives can be found in <a href=\"http://www2.imm.dtu.dk/pubdb/views/edoc_download.php/3274/pdf/imm3274.pdf\">Chapter 2 of The Matrix Cookbook</a>. For example, there you may find that $\\frac{\\partial}{\\partial A}\\log |A| = A^{-T}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A31OYSogBED6"
   },
   "source": [
    "\n",
    "<b>Task 2:</b> Implement M-step for GMM using template below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mhOr5I1bBED7"
   },
   "outputs": [],
   "source": [
    "def M_step(X, gamma):\n",
    "    \"\"\"\n",
    "    Performs M-step on GMM model\n",
    "    Each input is numpy array:\n",
    "    X: (N x d), data points\n",
    "    gamma: (N x C), distribution q(T)  \n",
    "    \n",
    "    Returns:\n",
    "    pi: (C)\n",
    "    mu: (C x d)\n",
    "    sigma: (C x d x d)\n",
    "    \"\"\"\n",
    "    N = X.shape[0] # number of objects\n",
    "    C = gamma.shape[1] # number of clusters\n",
    "    d = X.shape[1] # dimension of each object\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "    sigma = np.zeros((C, d, d))\n",
    "    print_shape_of(sigma, 'sigma')\n",
    "    \n",
    "    pi = np.mean(gamma, axis = 0)\n",
    "    print_shape_of(pi, 'pi')\n",
    "    \n",
    "    mu = np.dot(gamma.T, X) / np.sum(gamma, axis = 0)[:,np.newaxis]\n",
    "    print_shape_of(mu, 'mu')\n",
    "    \n",
    "    gamma_sum = np.sum(gamma, axis=0)[:, np.newaxis]\n",
    "    print_shape_of(gamma_sum, 'gamma_sum2')\n",
    "    \n",
    "    # NOTE C == 3\n",
    "    for ix in range(C):\n",
    "      x = X - mu[ix, :]\n",
    "      print_shape_of(x, 'x')\n",
    "        \n",
    "      gamma_diag = np.matrix(np.diag(gamma[:, ix]))\n",
    "      print_shape_of(gamma_diag, 'gamma_diag')\n",
    "        \n",
    "      sigma_per_class = x.T * gamma_diag * x\n",
    "      print_shape_of(sigma_per_class, 'sigma_per_class')\n",
    "        \n",
    "      sigma[ix, :, :] = (sigma_per_class) / gamma_sum[ix]\n",
    "\n",
    "    return pi, mu, sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "id": "6i5cv65SBED-",
    "outputId": "6c415baa-dd94-45ae-ea4f-e236d7ceda83"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current answer for task Task 2 (M-step: mu) is: 2.899391882050383\n",
      "Current answer for task Task 2 (M-step: sigma) is: 5.977105216897526\n",
      "Current answer for task Task 2 (M-step: pi) is: 0.5507624459218776\n"
     ]
    }
   ],
   "source": [
    "gamma = E_step(X, pi0, mu0, sigma0)\n",
    "pi, mu, sigma = M_step(X, gamma)\n",
    "grader.submit_m_step(pi, mu, sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "svyzjt7XBEEC"
   },
   "source": [
    "### Loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qexOXBLUBEED"
   },
   "source": [
    "Finally, we need some function to track convergence. We will use variational lower bound $\\mathcal{L}$ for this purpose. We will stop our EM iterations when $\\mathcal{L}$ will saturate. Usually, you will need only about 10-20 iterations to converge. It is also useful to check that this function never decreases during training. If it does, you have a bug in your code.\n",
    "\n",
    "<b>Task 3:</b> Implement a function that will compute $\\mathcal{L}$ using template below.\n",
    "\n",
    "$$\\mathcal{L} = \\sum_{i=1}^{N} \\sum_{c=1}^{C} q(t_i =c) (\\log \\pi_c + \\log f_{\\!\\mathcal{N}}(x_i \\mid \\mu_c, \\Sigma_c)) - \\sum_{i=1}^{N} \\sum_{c=1}^{K} q(t_i =c) \\log q(t_i =c)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B5tKCZe0BEEE"
   },
   "outputs": [],
   "source": [
    "def compute_vlb(X, pi, mu, sigma, gamma):\n",
    "    \"\"\"\n",
    "    Each input is numpy array:\n",
    "    X: (N x d), data points\n",
    "    gamma: (N x C), distribution q(T)  \n",
    "    pi: (C)\n",
    "    mu: (C x d)\n",
    "    sigma: (C x d x d)\n",
    "    \n",
    "    Returns value of variational lower bound\n",
    "    \"\"\"\n",
    "    N = X.shape[0] # number of objects\n",
    "    C = gamma.shape[1] # number of clusters\n",
    "    d = X.shape[1] # dimension of each object\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "    Eps1 = 1.e-20\n",
    "    Eps2 = Eps1 / 10.\n",
    "    \n",
    "    loss = np.zeros((N, C)) # N == 280, C == 3\n",
    "    for ix in range(C):\n",
    "      dist = mv_norm(mu[ix], sigma[ix], allow_singular=True)   \n",
    "      loss[:, ix] = gamma[:, ix] * (np.log(pi[ix] + Eps1) + dist.logpdf(X) - np.log(gamma[:, ix] + Eps2))\n",
    "      \n",
    "    \n",
    "    loss = np.sum(loss)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "xNva3XRTBEEI",
    "outputId": "a157f327-9c93-4d0a-9417-949f5d64a016"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current answer for task Task 3 (VLB) is: -1213.9734643060183\n"
     ]
    }
   ],
   "source": [
    "pi, mu, sigma = pi0, mu0, sigma0\n",
    "gamma = E_step(X, pi, mu, sigma)\n",
    "pi, mu, sigma = M_step(X, gamma)\n",
    "loss = compute_vlb(X, pi, mu, sigma, gamma)\n",
    "grader.submit_VLB(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I5p8XC-eBEEM"
   },
   "source": [
    "### Bringing it all together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cQsdsNVCBEEU"
   },
   "source": [
    "Now that we have E step, M step and VLB, we can implement the training loop. We will initialize values of $\\pi$, $\\mu$ and $\\Sigma$ to some random numbers, train until $\\mathcal{L}$ stops changing, and return the resulting points. We also know that the EM algorithm converges to local optima. To find a better local optima, we will restart the algorithm multiple times from different (random) starting positions. Each training trial should stop either when maximum number of iterations is reached or when relative improvement is smaller than given tolerance ($|\\frac{\\mathcal{L}_i-\\mathcal{L}_{i-1}}{\\mathcal{L}_{i-1}}| \\le \\text{rtol}$).\n",
    "\n",
    "Remember, that initial (random) values of $\\pi$ that you generate must be non-negative and sum up to 1. Also, $\\Sigma$ matrices must be symmetric and positive semi-definite. If you don't know how to generate those matrices, you can use $\\Sigma=I$ as initialization.\n",
    "\n",
    "You will also sometimes get numerical errors because of component collapsing. The easiest way to deal with this problems is to restart the procedure.\n",
    "\n",
    "<b>Task 4:</b> Implement training procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Q1AAcyl0BEEW"
   },
   "outputs": [],
   "source": [
    "ENABLED = False\n",
    "\n",
    "def init_sigma_random(d, c):\n",
    "  sigma = np.zeros((c, d, d))\n",
    "  _a = np.random.rand(c, d, d)\n",
    "  \n",
    "  for ix in range(c):\n",
    "    _a[ix, :, : ] = np.dot(_a[ix, :, :], _a[ix, :, :].T) \n",
    "    sigma[ix] = np.tril(_a[ix, :, :]) + np.tril(_a[ix, :, :], -1).T\n",
    "    \n",
    "  print_shape_of(sigma, 'sigma')\n",
    "  return sigma\n",
    "\n",
    "def init_sigma_id(d, c):\n",
    "  sigma = np.zeros((c, d, d))\n",
    "  \n",
    "  for ix in range(c):\n",
    "    sigma[ix] = np.identity(d)\n",
    "    \n",
    "  print_shape_of(sigma, 'sigma')\n",
    "  return sigma\n",
    "\n",
    "def init_parms(n, d, c, strategy='random'):\n",
    "  # print(\"in init_parms - 1\")\n",
    "  pi = np.random.rand(c) \n",
    "  pi /= np.sum(pi)\n",
    "  assert np.all(np.abs(np.sum(pi, axis=0) - 1.0) <= 0.000001), \"any line of gamma matrix should sum to 1.0\"\n",
    "  \n",
    "  mu = np.random.rand(c, d)\n",
    "  sigma = init_sigma_id(d, c) # init_sigma(d, c)\n",
    "  \n",
    "  return pi, mu, sigma\n",
    "\n",
    "\n",
    "def best(val, best):\n",
    "  if best is None:\n",
    "    best = val\n",
    "  return best if best > val else val\n",
    "\n",
    "def train_EM(X, C, rtol=1e-3, max_iter=100, restarts=10):\n",
    "    '''\n",
    "    Starts with random initialization *restarts* times\n",
    "    Runs optimization until saturation with *rtol* reached\n",
    "    or *max_iter* iterations were made.\n",
    "    \n",
    "    X: (N, d), data points\n",
    "    C: int, number of clusters\n",
    "    '''\n",
    "    N = X.shape[0] # number of objects\n",
    "    d = X.shape[1] # dimension of each object\n",
    "    best_loss = None\n",
    "    best_pi = None\n",
    "    best_mu = None\n",
    "    best_sigma = None \n",
    "    \n",
    "    for r in range(restarts):\n",
    "        try:\n",
    "            ### YOUR CODE HERE\n",
    "            best_loss = -1.0e10 if best_loss is None else best_loss \n",
    "            pi, mu, sigma = init_parms(N, d, C, strategy='random')\n",
    "            ploss = None\n",
    "            print(\"range \" + str(r))\n",
    "            \n",
    "            for iter in range(max_iter):\n",
    "              ploss = 1 if ploss is None else ploss\n",
    "              gamma = E_step(X, pi, mu, sigma)\n",
    "              pi, mu, sigma = M_step(X, gamma)\n",
    "              closs = compute_vlb(X, pi, mu, sigma, gamma)\n",
    "              # print(\"\\t==> current loss[\" + str(iter) + \"]: \" + str(closs) + \" / ploss: \" + str(ploss) + \" / best_loss: \" + str(best_loss)) \n",
    "              \n",
    "              if abs((closs - ploss) / ploss) <= rtol:\n",
    "                break\n",
    "              else:\n",
    "                ploss = closs\n",
    "            \n",
    "              # keep track of best_loss over restart\n",
    "              if closs > best_loss:\n",
    "                print(\"\\tComp prev(abs(best_loss)): \" + str(abs(best_loss)) + \" / abs(closs): \" + str(abs(closs)))\n",
    "                best_pi = pi \n",
    "                best_mu = mu\n",
    "                best_sigma = sigma\n",
    "                best_loss = closs\n",
    "                \n",
    "              \n",
    "\n",
    "        except np.linalg.LinAlgError:\n",
    "            print(\"Singular matrix: components collapsed\")\n",
    "            pass\n",
    "\n",
    "    return best_loss, best_pi, best_mu, best_sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 4278
    },
    "colab_type": "code",
    "id": "0f8A5sbmBEEZ",
    "outputId": "1fcd5225-8f1c-4d9c-948e-015ba97223dd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "range 0\n",
      "\t==> current loss[0]: -1208.7000661092363 / ploss: 1 / best_loss: -10000000000.0\n",
      "\tComp prev(abs(best_loss)): 10000000000.0 / abs(closs): 1208.7000661092363\n",
      "\t==> current loss[1]: -1184.2670317433394 / ploss: -1208.7000661092363 / best_loss: -1208.7000661092363\n",
      "\tComp prev(abs(best_loss)): 1208.7000661092363 / abs(closs): 1184.2670317433394\n",
      "\t==> current loss[2]: -1169.1202411706818 / ploss: -1184.2670317433394 / best_loss: -1184.2670317433394\n",
      "\tComp prev(abs(best_loss)): 1184.2670317433394 / abs(closs): 1169.1202411706818\n",
      "\t==> current loss[3]: -1155.803026490948 / ploss: -1169.1202411706818 / best_loss: -1169.1202411706818\n",
      "\tComp prev(abs(best_loss)): 1169.1202411706818 / abs(closs): 1155.803026490948\n",
      "\t==> current loss[4]: -1144.742206956782 / ploss: -1155.803026490948 / best_loss: -1155.803026490948\n",
      "\tComp prev(abs(best_loss)): 1155.803026490948 / abs(closs): 1144.742206956782\n",
      "\t==> current loss[5]: -1135.7261189698456 / ploss: -1144.742206956782 / best_loss: -1144.742206956782\n",
      "\tComp prev(abs(best_loss)): 1144.742206956782 / abs(closs): 1135.7261189698456\n",
      "\t==> current loss[6]: -1128.0690358370873 / ploss: -1135.7261189698456 / best_loss: -1135.7261189698456\n",
      "\tComp prev(abs(best_loss)): 1135.7261189698456 / abs(closs): 1128.0690358370873\n",
      "\t==> current loss[7]: -1121.4948074780975 / ploss: -1128.0690358370873 / best_loss: -1128.0690358370873\n",
      "\tComp prev(abs(best_loss)): 1128.0690358370873 / abs(closs): 1121.4948074780975\n",
      "\t==> current loss[8]: -1115.77460377999 / ploss: -1121.4948074780975 / best_loss: -1121.4948074780975\n",
      "\tComp prev(abs(best_loss)): 1121.4948074780975 / abs(closs): 1115.77460377999\n",
      "\t==> current loss[9]: -1110.6166137319578 / ploss: -1115.77460377999 / best_loss: -1115.77460377999\n",
      "\tComp prev(abs(best_loss)): 1115.77460377999 / abs(closs): 1110.6166137319578\n",
      "\t==> current loss[10]: -1105.8690727494266 / ploss: -1110.6166137319578 / best_loss: -1110.6166137319578\n",
      "\tComp prev(abs(best_loss)): 1110.6166137319578 / abs(closs): 1105.8690727494266\n",
      "\t==> current loss[11]: -1101.6161284126256 / ploss: -1105.8690727494266 / best_loss: -1105.8690727494266\n",
      "\tComp prev(abs(best_loss)): 1105.8690727494266 / abs(closs): 1101.6161284126256\n",
      "\t==> current loss[12]: -1097.9934522679964 / ploss: -1101.6161284126256 / best_loss: -1101.6161284126256\n",
      "\tComp prev(abs(best_loss)): 1101.6161284126256 / abs(closs): 1097.9934522679964\n",
      "\t==> current loss[13]: -1094.823265955652 / ploss: -1097.9934522679964 / best_loss: -1097.9934522679964\n",
      "\tComp prev(abs(best_loss)): 1097.9934522679964 / abs(closs): 1094.823265955652\n",
      "\t==> current loss[14]: -1091.5351809890765 / ploss: -1094.823265955652 / best_loss: -1094.823265955652\n",
      "\tComp prev(abs(best_loss)): 1094.823265955652 / abs(closs): 1091.5351809890765\n",
      "\t==> current loss[15]: -1087.3431721661539 / ploss: -1091.5351809890765 / best_loss: -1091.5351809890765\n",
      "\tComp prev(abs(best_loss)): 1091.5351809890765 / abs(closs): 1087.3431721661539\n",
      "\t==> current loss[16]: -1082.0153371609485 / ploss: -1087.3431721661539 / best_loss: -1087.3431721661539\n",
      "\tComp prev(abs(best_loss)): 1087.3431721661539 / abs(closs): 1082.0153371609485\n",
      "\t==> current loss[17]: -1075.903188610755 / ploss: -1082.0153371609485 / best_loss: -1082.0153371609485\n",
      "\tComp prev(abs(best_loss)): 1082.0153371609485 / abs(closs): 1075.903188610755\n",
      "\t==> current loss[18]: -1067.2518675326387 / ploss: -1075.903188610755 / best_loss: -1075.903188610755\n",
      "\tComp prev(abs(best_loss)): 1075.903188610755 / abs(closs): 1067.2518675326387\n",
      "\t==> current loss[19]: -1064.172073006309 / ploss: -1067.2518675326387 / best_loss: -1067.2518675326387\n",
      "\tComp prev(abs(best_loss)): 1067.2518675326387 / abs(closs): 1064.172073006309\n",
      "\t==> current loss[20]: -1063.911144220174 / ploss: -1064.172073006309 / best_loss: -1064.172073006309\n",
      "range 1\n",
      "\t==> current loss[0]: -1223.3935487623398 / ploss: 1 / best_loss: -1064.172073006309\n",
      "\t==> current loss[1]: -1206.793563031346 / ploss: -1223.3935487623398 / best_loss: -1064.172073006309\n",
      "\t==> current loss[2]: -1185.5268427283277 / ploss: -1206.793563031346 / best_loss: -1064.172073006309\n",
      "\t==> current loss[3]: -1143.7206921651632 / ploss: -1185.5268427283277 / best_loss: -1064.172073006309\n",
      "\t==> current loss[4]: -1092.4492574450849 / ploss: -1143.7206921651632 / best_loss: -1064.172073006309\n",
      "\t==> current loss[5]: -1077.864578731927 / ploss: -1092.4492574450849 / best_loss: -1064.172073006309\n",
      "\t==> current loss[6]: -1073.2794324012586 / ploss: -1077.864578731927 / best_loss: -1064.172073006309\n",
      "\t==> current loss[7]: -1069.1850712848059 / ploss: -1073.2794324012586 / best_loss: -1064.172073006309\n",
      "\t==> current loss[8]: -1066.0102801706698 / ploss: -1069.1850712848059 / best_loss: -1064.172073006309\n",
      "\t==> current loss[9]: -1064.6600119241966 / ploss: -1066.0102801706698 / best_loss: -1064.172073006309\n",
      "\t==> current loss[10]: -1064.1710449175916 / ploss: -1064.6600119241966 / best_loss: -1064.172073006309\n",
      "range 2\n",
      "\t==> current loss[0]: -1200.8233910327833 / ploss: 1 / best_loss: -1064.172073006309\n",
      "\t==> current loss[1]: -1177.5544986978996 / ploss: -1200.8233910327833 / best_loss: -1064.172073006309\n",
      "\t==> current loss[2]: -1166.6853654026697 / ploss: -1177.5544986978996 / best_loss: -1064.172073006309\n",
      "\t==> current loss[3]: -1155.5228320461565 / ploss: -1166.6853654026697 / best_loss: -1064.172073006309\n",
      "\t==> current loss[4]: -1144.5486745228563 / ploss: -1155.5228320461565 / best_loss: -1064.172073006309\n",
      "\t==> current loss[5]: -1135.8397041340436 / ploss: -1144.5486745228563 / best_loss: -1064.172073006309\n",
      "\t==> current loss[6]: -1129.5912960325695 / ploss: -1135.8397041340436 / best_loss: -1064.172073006309\n",
      "\t==> current loss[7]: -1124.3063967837838 / ploss: -1129.5912960325695 / best_loss: -1064.172073006309\n",
      "\t==> current loss[8]: -1119.3920100331165 / ploss: -1124.3063967837838 / best_loss: -1064.172073006309\n",
      "\t==> current loss[9]: -1114.7755222541177 / ploss: -1119.3920100331165 / best_loss: -1064.172073006309\n",
      "\t==> current loss[10]: -1110.40862269226 / ploss: -1114.7755222541177 / best_loss: -1064.172073006309\n",
      "\t==> current loss[11]: -1106.3636661536543 / ploss: -1110.40862269226 / best_loss: -1064.172073006309\n",
      "\t==> current loss[12]: -1102.8733000134425 / ploss: -1106.3636661536543 / best_loss: -1064.172073006309\n",
      "\t==> current loss[13]: -1100.103289486898 / ploss: -1102.8733000134425 / best_loss: -1064.172073006309\n",
      "\t==> current loss[14]: -1097.9233809455914 / ploss: -1100.103289486898 / best_loss: -1064.172073006309\n",
      "\t==> current loss[15]: -1095.9841559598456 / ploss: -1097.9233809455914 / best_loss: -1064.172073006309\n",
      "\t==> current loss[16]: -1093.9213858916326 / ploss: -1095.9841559598456 / best_loss: -1064.172073006309\n",
      "\t==> current loss[17]: -1091.356328551952 / ploss: -1093.9213858916326 / best_loss: -1064.172073006309\n",
      "\t==> current loss[18]: -1087.767641502459 / ploss: -1091.356328551952 / best_loss: -1064.172073006309\n",
      "\t==> current loss[19]: -1083.0012290631391 / ploss: -1087.767641502459 / best_loss: -1064.172073006309\n",
      "\t==> current loss[20]: -1077.4878853897462 / ploss: -1083.0012290631391 / best_loss: -1064.172073006309\n",
      "\t==> current loss[21]: -1069.3754160908238 / ploss: -1077.4878853897462 / best_loss: -1064.172073006309\n",
      "\t==> current loss[22]: -1064.3685101580943 / ploss: -1069.3754160908238 / best_loss: -1064.172073006309\n",
      "\t==> current loss[23]: -1063.9460714536217 / ploss: -1064.3685101580943 / best_loss: -1064.172073006309\n",
      "range 3\n",
      "\t==> current loss[0]: -1201.2971768634782 / ploss: 1 / best_loss: -1064.172073006309\n",
      "\t==> current loss[1]: -1170.476741198081 / ploss: -1201.2971768634782 / best_loss: -1064.172073006309\n",
      "\t==> current loss[2]: -1162.1317826325107 / ploss: -1170.476741198081 / best_loss: -1064.172073006309\n",
      "\t==> current loss[3]: -1159.2109796456293 / ploss: -1162.1317826325107 / best_loss: -1064.172073006309\n",
      "\t==> current loss[4]: -1157.487025584733 / ploss: -1159.2109796456293 / best_loss: -1064.172073006309\n",
      "\t==> current loss[5]: -1156.0826257466979 / ploss: -1157.487025584733 / best_loss: -1064.172073006309\n",
      "\t==> current loss[6]: -1154.5930123172798 / ploss: -1156.0826257466979 / best_loss: -1064.172073006309\n",
      "\t==> current loss[7]: -1152.686417541156 / ploss: -1154.5930123172798 / best_loss: -1064.172073006309\n",
      "\t==> current loss[8]: -1149.8752055919397 / ploss: -1152.686417541156 / best_loss: -1064.172073006309\n",
      "\t==> current loss[9]: -1145.2030021090948 / ploss: -1149.8752055919397 / best_loss: -1064.172073006309\n",
      "\t==> current loss[10]: -1136.8477025246293 / ploss: -1145.2030021090948 / best_loss: -1064.172073006309\n",
      "\t==> current loss[11]: -1123.0771818094383 / ploss: -1136.8477025246293 / best_loss: -1064.172073006309\n",
      "\t==> current loss[12]: -1107.8936823252498 / ploss: -1123.0771818094383 / best_loss: -1064.172073006309\n",
      "\t==> current loss[13]: -1099.1196412434751 / ploss: -1107.8936823252498 / best_loss: -1064.172073006309\n",
      "\t==> current loss[14]: -1094.38868623271 / ploss: -1099.1196412434751 / best_loss: -1064.172073006309\n",
      "\t==> current loss[15]: -1089.21033289233 / ploss: -1094.38868623271 / best_loss: -1064.172073006309\n",
      "\t==> current loss[16]: -1081.3226894432985 / ploss: -1089.21033289233 / best_loss: -1064.172073006309\n",
      "\t==> current loss[17]: -1070.867058456625 / ploss: -1081.3226894432985 / best_loss: -1064.172073006309\n",
      "\t==> current loss[18]: -1064.2058968539413 / ploss: -1070.867058456625 / best_loss: -1064.172073006309\n",
      "\t==> current loss[19]: -1063.8721670315456 / ploss: -1064.2058968539413 / best_loss: -1064.172073006309\n",
      "range 4\n",
      "\t==> current loss[0]: -1229.6871046406968 / ploss: 1 / best_loss: -1064.172073006309\n",
      "\t==> current loss[1]: -1203.4009999746195 / ploss: -1229.6871046406968 / best_loss: -1064.172073006309\n",
      "\t==> current loss[2]: -1171.0236882260533 / ploss: -1203.4009999746195 / best_loss: -1064.172073006309\n",
      "\t==> current loss[3]: -1159.5294102376029 / ploss: -1171.0236882260533 / best_loss: -1064.172073006309\n",
      "\t==> current loss[4]: -1156.748143509376 / ploss: -1159.5294102376029 / best_loss: -1064.172073006309\n",
      "\t==> current loss[5]: -1154.9838401963102 / ploss: -1156.748143509376 / best_loss: -1064.172073006309\n",
      "\t==> current loss[6]: -1152.9471376899805 / ploss: -1154.9838401963102 / best_loss: -1064.172073006309\n",
      "\t==> current loss[7]: -1149.9443487848012 / ploss: -1152.9471376899805 / best_loss: -1064.172073006309\n",
      "\t==> current loss[8]: -1144.8727714073211 / ploss: -1149.9443487848012 / best_loss: -1064.172073006309\n",
      "\t==> current loss[9]: -1136.261089178984 / ploss: -1144.8727714073211 / best_loss: -1064.172073006309\n",
      "\t==> current loss[10]: -1124.7839605261092 / ploss: -1136.261089178984 / best_loss: -1064.172073006309\n",
      "\t==> current loss[11]: -1114.9947931493398 / ploss: -1124.7839605261092 / best_loss: -1064.172073006309\n",
      "\t==> current loss[12]: -1109.055838103 / ploss: -1114.9947931493398 / best_loss: -1064.172073006309\n",
      "\t==> current loss[13]: -1105.0880922466042 / ploss: -1109.055838103 / best_loss: -1064.172073006309\n",
      "\t==> current loss[14]: -1102.0691064122866 / ploss: -1105.0880922466042 / best_loss: -1064.172073006309\n",
      "\t==> current loss[15]: -1099.7906135474766 / ploss: -1102.0691064122866 / best_loss: -1064.172073006309\n",
      "\t==> current loss[16]: -1097.980698830999 / ploss: -1099.7906135474766 / best_loss: -1064.172073006309\n",
      "\t==> current loss[17]: -1096.307395482811 / ploss: -1097.980698830999 / best_loss: -1064.172073006309\n",
      "\t==> current loss[18]: -1094.486226496928 / ploss: -1096.307395482811 / best_loss: -1064.172073006309\n",
      "\t==> current loss[19]: -1092.2398587541757 / ploss: -1094.486226496928 / best_loss: -1064.172073006309\n",
      "\t==> current loss[20]: -1089.1381270198112 / ploss: -1092.2398587541757 / best_loss: -1064.172073006309\n",
      "\t==> current loss[21]: -1084.81360984921 / ploss: -1089.1381270198112 / best_loss: -1064.172073006309\n",
      "\t==> current loss[22]: -1079.7056897844618 / ploss: -1084.81360984921 / best_loss: -1064.172073006309\n",
      "\t==> current loss[23]: -1072.884585330814 / ploss: -1079.7056897844618 / best_loss: -1064.172073006309\n",
      "\t==> current loss[24]: -1065.143013901176 / ploss: -1072.884585330814 / best_loss: -1064.172073006309\n",
      "\t==> current loss[25]: -1064.0261133334013 / ploss: -1065.143013901176 / best_loss: -1064.172073006309\n",
      "\tComp prev(abs(best_loss)): 1064.172073006309 / abs(closs): 1064.0261133334013\n",
      "\t==> current loss[26]: -1063.8740762531488 / ploss: -1064.0261133334013 / best_loss: -1064.0261133334013\n",
      "range 5\n",
      "\t==> current loss[0]: -1219.7286817332538 / ploss: 1 / best_loss: -1064.0261133334013\n",
      "\t==> current loss[1]: -1196.6530842270474 / ploss: -1219.7286817332538 / best_loss: -1064.0261133334013\n",
      "\t==> current loss[2]: -1182.7672735389206 / ploss: -1196.6530842270474 / best_loss: -1064.0261133334013\n",
      "\t==> current loss[3]: -1174.1099331920552 / ploss: -1182.7672735389206 / best_loss: -1064.0261133334013\n",
      "\t==> current loss[4]: -1167.285601531701 / ploss: -1174.1099331920552 / best_loss: -1064.0261133334013\n",
      "\t==> current loss[5]: -1159.8382507090546 / ploss: -1167.285601531701 / best_loss: -1064.0261133334013\n",
      "\t==> current loss[6]: -1148.9855012686967 / ploss: -1159.8382507090546 / best_loss: -1064.0261133334013\n",
      "\t==> current loss[7]: -1132.5941628212404 / ploss: -1148.9855012686967 / best_loss: -1064.0261133334013\n",
      "\t==> current loss[8]: -1115.6437300723483 / ploss: -1132.5941628212404 / best_loss: -1064.0261133334013\n",
      "\t==> current loss[9]: -1104.578108691089 / ploss: -1115.6437300723483 / best_loss: -1064.0261133334013\n",
      "\t==> current loss[10]: -1097.5166516079557 / ploss: -1104.578108691089 / best_loss: -1064.0261133334013\n",
      "\t==> current loss[11]: -1092.3591152687707 / ploss: -1097.5166516079557 / best_loss: -1064.0261133334013\n",
      "\t==> current loss[12]: -1088.108753193928 / ploss: -1092.3591152687707 / best_loss: -1064.0261133334013\n",
      "\t==> current loss[13]: -1084.2526945826191 / ploss: -1088.108753193928 / best_loss: -1064.0261133334013\n",
      "\t==> current loss[14]: -1080.6471724285539 / ploss: -1084.2526945826191 / best_loss: -1064.0261133334013\n",
      "\t==> current loss[15]: -1075.9106152631743 / ploss: -1080.6471724285539 / best_loss: -1064.0261133334013\n",
      "\t==> current loss[16]: -1068.2945448644023 / ploss: -1075.9106152631743 / best_loss: -1064.0261133334013\n",
      "\t==> current loss[17]: -1064.5182502227922 / ploss: -1068.2945448644023 / best_loss: -1064.0261133334013\n",
      "\t==> current loss[18]: -1064.0356153479097 / ploss: -1064.5182502227922 / best_loss: -1064.0261133334013\n",
      "range 6\n",
      "\t==> current loss[0]: -1201.2516081814144 / ploss: 1 / best_loss: -1064.0261133334013\n",
      "\t==> current loss[1]: -1171.3620337293678 / ploss: -1201.2516081814144 / best_loss: -1064.0261133334013\n",
      "\t==> current loss[2]: -1165.2248173482092 / ploss: -1171.3620337293678 / best_loss: -1064.0261133334013\n",
      "\t==> current loss[3]: -1162.241054759444 / ploss: -1165.2248173482092 / best_loss: -1064.0261133334013\n",
      "\t==> current loss[4]: -1157.1327065658052 / ploss: -1162.241054759444 / best_loss: -1064.0261133334013\n",
      "\t==> current loss[5]: -1145.5006595530417 / ploss: -1157.1327065658052 / best_loss: -1064.0261133334013\n",
      "\t==> current loss[6]: -1130.8960809878672 / ploss: -1145.5006595530417 / best_loss: -1064.0261133334013\n",
      "\t==> current loss[7]: -1128.0174790769258 / ploss: -1130.8960809878672 / best_loss: -1064.0261133334013\n",
      "\t==> current loss[8]: -1126.4275061741882 / ploss: -1128.0174790769258 / best_loss: -1064.0261133334013\n",
      "\t==> current loss[9]: -1125.0074495726421 / ploss: -1126.4275061741882 / best_loss: -1064.0261133334013\n",
      "\t==> current loss[10]: -1123.6915254053056 / ploss: -1125.0074495726421 / best_loss: -1064.0261133334013\n",
      "\t==> current loss[11]: -1122.2703187889101 / ploss: -1123.6915254053056 / best_loss: -1064.0261133334013\n",
      "\t==> current loss[12]: -1120.4187300452822 / ploss: -1122.2703187889101 / best_loss: -1064.0261133334013\n",
      "\t==> current loss[13]: -1117.5734383826637 / ploss: -1120.4187300452822 / best_loss: -1064.0261133334013\n",
      "\t==> current loss[14]: -1112.55045202243 / ploss: -1117.5734383826637 / best_loss: -1064.0261133334013\n",
      "\t==> current loss[15]: -1102.9225871465471 / ploss: -1112.55045202243 / best_loss: -1064.0261133334013\n",
      "\t==> current loss[16]: -1086.3983962117695 / ploss: -1102.9225871465471 / best_loss: -1064.0261133334013\n",
      "\t==> current loss[17]: -1070.5169877999579 / ploss: -1086.3983962117695 / best_loss: -1064.0261133334013\n",
      "\t==> current loss[18]: -1065.0525417684437 / ploss: -1070.5169877999579 / best_loss: -1064.0261133334013\n",
      "\t==> current loss[19]: -1064.061724013153 / ploss: -1065.0525417684437 / best_loss: -1064.0261133334013\n",
      "range 7\n",
      "\t==> current loss[0]: -1235.204994276543 / ploss: 1 / best_loss: -1064.0261133334013\n",
      "\t==> current loss[1]: -1232.2497875323268 / ploss: -1235.204994276543 / best_loss: -1064.0261133334013\n",
      "\t==> current loss[2]: -1227.6310113606314 / ploss: -1232.2497875323268 / best_loss: -1064.0261133334013\n",
      "\t==> current loss[3]: -1216.111257943735 / ploss: -1227.6310113606314 / best_loss: -1064.0261133334013\n",
      "\t==> current loss[4]: -1202.023512789391 / ploss: -1216.111257943735 / best_loss: -1064.0261133334013\n",
      "\t==> current loss[5]: -1192.613440775583 / ploss: -1202.023512789391 / best_loss: -1064.0261133334013\n",
      "\t==> current loss[6]: -1186.2404114900862 / ploss: -1192.613440775583 / best_loss: -1064.0261133334013\n",
      "\t==> current loss[7]: -1181.4304131380231 / ploss: -1186.2404114900862 / best_loss: -1064.0261133334013\n",
      "\t==> current loss[8]: -1177.4212430419657 / ploss: -1181.4304131380231 / best_loss: -1064.0261133334013\n",
      "\t==> current loss[9]: -1173.5065873105495 / ploss: -1177.4212430419657 / best_loss: -1064.0261133334013\n",
      "\t==> current loss[10]: -1168.5558729946952 / ploss: -1173.5065873105495 / best_loss: -1064.0261133334013\n",
      "\t==> current loss[11]: -1160.4517471043746 / ploss: -1168.5558729946952 / best_loss: -1064.0261133334013\n",
      "\t==> current loss[12]: -1148.1630829511837 / ploss: -1160.4517471043746 / best_loss: -1064.0261133334013\n",
      "\t==> current loss[13]: -1137.5524667941331 / ploss: -1148.1630829511837 / best_loss: -1064.0261133334013\n",
      "\t==> current loss[14]: -1130.7681173188407 / ploss: -1137.5524667941331 / best_loss: -1064.0261133334013\n",
      "\t==> current loss[15]: -1124.718619082625 / ploss: -1130.7681173188407 / best_loss: -1064.0261133334013\n",
      "\t==> current loss[16]: -1117.9626186374599 / ploss: -1124.718619082625 / best_loss: -1064.0261133334013\n",
      "\t==> current loss[17]: -1109.9966960751894 / ploss: -1117.9626186374599 / best_loss: -1064.0261133334013\n",
      "\t==> current loss[18]: -1100.5542109050002 / ploss: -1109.9966960751894 / best_loss: -1064.0261133334013\n",
      "\t==> current loss[19]: -1091.0074441436002 / ploss: -1100.5542109050002 / best_loss: -1064.0261133334013\n",
      "\t==> current loss[20]: -1083.1060302041806 / ploss: -1091.0074441436002 / best_loss: -1064.0261133334013\n",
      "\t==> current loss[21]: -1077.6836819573318 / ploss: -1083.1060302041806 / best_loss: -1064.0261133334013\n",
      "\t==> current loss[22]: -1074.0780051376205 / ploss: -1077.6836819573318 / best_loss: -1064.0261133334013\n",
      "\t==> current loss[23]: -1071.2530097298306 / ploss: -1074.0780051376205 / best_loss: -1064.0261133334013\n",
      "\t==> current loss[24]: -1068.7633674266976 / ploss: -1071.2530097298306 / best_loss: -1064.0261133334013\n",
      "\t==> current loss[25]: -1066.6657954983984 / ploss: -1068.7633674266976 / best_loss: -1064.0261133334013\n",
      "\t==> current loss[26]: -1065.1727192602684 / ploss: -1066.6657954983984 / best_loss: -1064.0261133334013\n",
      "\t==> current loss[27]: -1064.3559416105902 / ploss: -1065.1727192602684 / best_loss: -1064.0261133334013\n",
      "range 8\n",
      "\t==> current loss[0]: -1228.0882931704627 / ploss: 1 / best_loss: -1064.0261133334013\n",
      "\t==> current loss[1]: -1203.3029334689725 / ploss: -1228.0882931704627 / best_loss: -1064.0261133334013\n",
      "\t==> current loss[2]: -1168.8843309325896 / ploss: -1203.3029334689725 / best_loss: -1064.0261133334013\n",
      "\t==> current loss[3]: -1148.0986668835737 / ploss: -1168.8843309325896 / best_loss: -1064.0261133334013\n",
      "\t==> current loss[4]: -1132.6339359853428 / ploss: -1148.0986668835737 / best_loss: -1064.0261133334013\n",
      "\t==> current loss[5]: -1108.6472942174682 / ploss: -1132.6339359853428 / best_loss: -1064.0261133334013\n",
      "\t==> current loss[6]: -1077.25299836743 / ploss: -1108.6472942174682 / best_loss: -1064.0261133334013\n",
      "\t==> current loss[7]: -1067.5057048274252 / ploss: -1077.25299836743 / best_loss: -1064.0261133334013\n",
      "\t==> current loss[8]: -1065.5269752240547 / ploss: -1067.5057048274252 / best_loss: -1064.0261133334013\n",
      "\t==> current loss[9]: -1064.5921007302536 / ploss: -1065.5269752240547 / best_loss: -1064.0261133334013\n",
      "range 9\n",
      "\t==> current loss[0]: -1225.5310591269833 / ploss: 1 / best_loss: -1064.0261133334013\n",
      "\t==> current loss[1]: -1194.8584050344125 / ploss: -1225.5310591269833 / best_loss: -1064.0261133334013\n",
      "\t==> current loss[2]: -1166.4447225968834 / ploss: -1194.8584050344125 / best_loss: -1064.0261133334013\n",
      "\t==> current loss[3]: -1147.7238625252853 / ploss: -1166.4447225968834 / best_loss: -1064.0261133334013\n",
      "\t==> current loss[4]: -1135.1750824340515 / ploss: -1147.7238625252853 / best_loss: -1064.0261133334013\n",
      "\t==> current loss[5]: -1126.3731089235628 / ploss: -1135.1750824340515 / best_loss: -1064.0261133334013\n",
      "\t==> current loss[6]: -1119.6667203368675 / ploss: -1126.3731089235628 / best_loss: -1064.0261133334013\n",
      "\t==> current loss[7]: -1114.119612080903 / ploss: -1119.6667203368675 / best_loss: -1064.0261133334013\n",
      "\t==> current loss[8]: -1109.2173082198183 / ploss: -1114.119612080903 / best_loss: -1064.0261133334013\n",
      "\t==> current loss[9]: -1104.8180210836522 / ploss: -1109.2173082198183 / best_loss: -1064.0261133334013\n",
      "\t==> current loss[10]: -1101.045115187141 / ploss: -1104.8180210836522 / best_loss: -1064.0261133334013\n",
      "\t==> current loss[11]: -1097.9707121099657 / ploss: -1101.045115187141 / best_loss: -1064.0261133334013\n",
      "\t==> current loss[12]: -1095.3296520261547 / ploss: -1097.9707121099657 / best_loss: -1064.0261133334013\n",
      "\t==> current loss[13]: -1092.6158176420295 / ploss: -1095.3296520261547 / best_loss: -1064.0261133334013\n",
      "\t==> current loss[14]: -1089.192563077563 / ploss: -1092.6158176420295 / best_loss: -1064.0261133334013\n",
      "\t==> current loss[15]: -1084.546060603359 / ploss: -1089.192563077563 / best_loss: -1064.0261133334013\n",
      "\t==> current loss[16]: -1079.1273951841697 / ploss: -1084.546060603359 / best_loss: -1064.0261133334013\n",
      "\t==> current loss[17]: -1071.8478607676643 / ploss: -1079.1273951841697 / best_loss: -1064.0261133334013\n",
      "\t==> current loss[18]: -1064.7876466331625 / ploss: -1071.8478607676643 / best_loss: -1064.0261133334013\n",
      "\t==> current loss[19]: -1063.9939305053883 / ploss: -1064.7876466331625 / best_loss: -1064.0261133334013\n",
      "Current answer for task Task 4 (EM) is: -1064.0261133334013\n"
     ]
    }
   ],
   "source": [
    "best_loss, best_pi, best_mu, best_sigma = train_EM(X, 3)\n",
    "grader.submit_EM(best_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tYd6tPHKBEEd"
   },
   "source": [
    "If you implemented all the steps correctly, your algorithm should converge in about 20 iterations. Let's plot the clusters to see it. We will assign a cluster label as the most probable cluster index. This can be found using a matrix $\\gamma$ computed on last E-step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "colab_type": "code",
    "id": "mK_M6QLnBEEe",
    "outputId": "cc09eed1-b1f4-4949-a7b9-149a6c5c13cd"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW8AAAD4CAYAAAAjKGdbAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nbxN6AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "gamma = E_step(X, best_pi, best_mu, best_sigma)\n",
    "labels = gamma.argmax(axis=1)\n",
    "colors = np.array([(31, 119, 180), (255, 127, 14), (44, 160, 44)]) / 255.\n",
    "plt.scatter(X[:, 0], X[:, 1], c=colors[labels], s=30)\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "h4lnjrc7BEEl"
   },
   "source": [
    "# Authorization & Submission\n",
    "To submit assignment parts to Cousera platform, please, enter your e-mail and token into variables below. You can generate a token on this programming assignment's page. <b>Note:</b> The token expires 30 minutes after generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 146
    },
    "colab_type": "code",
    "id": "5uQRl29_BEEl",
    "outputId": "1600034b-978e-47f5-9648-a7b856a3990a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You want to submit these numbers:\n",
      "Task Task 1 (E-step): 0.5337178741081263\n",
      "Task Task 2 (M-step: mu): 2.899391882050383\n",
      "Task Task 2 (M-step: sigma): 5.977105216897526\n",
      "Task Task 2 (M-step: pi): 0.5507624459218776\n",
      "Task Task 3 (VLB): -1213.9734643060183\n",
      "Task Task 4 (EM): -1064.0261133334013\n"
     ]
    }
   ],
   "source": [
    "STUDENT_EMAIL = 'XXXXXXXXXXXXXXXXXXX' # EMAIL HERE\n",
    "STUDENT_TOKEN = 'XXXXXXXXXXXXXXXXXXX' # TOKEN HERE\n",
    "grader.status()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sNCsqNOiBEEn"
   },
   "source": [
    "If you want to submit these answers, run cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "3sc9imWNBEEo",
    "outputId": "6064f29d-f8f1-48fe-e225-b47d97fa5a0b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted to Coursera platform. See results on assignment page!\n"
     ]
    }
   ],
   "source": [
    "grader.submit(STUDENT_EMAIL, STUDENT_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V0-TvkW742z5"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of em_assignment.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
